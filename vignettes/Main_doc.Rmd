---
title: "Intership report"
subtitle: "NoiseCapture's database analysis"
author: "Ludovic Moisan"
supervisors: "Pierre Aumond, Paul Chapron, Nicolas Roelandt"
date: "`r Sys.Date()`"
output: 
  html_document :
    toc: TRUE
    toc_float: TRUE
    theme: united
editor_options: 
  chunk_output_type: console
---

<style>
body {
text-align: justify}
</style>

```{r document-generation, include = FALSE}

#> If fast == True, then all big rds will be downloaded instead of being calculated on the fly.
#> Only some few calculations, the texts and plots will be generated by the knitting, 
#> which will accurately produce the same results we have.
#> If fast == False, generating every data frame and downloading weather info takes about 2h-3h on a laptop with 12Go ram I5 8th gen and a good internet connection

fast <- TRUE

```

```{r setup-library, include= FALSE}

knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)

# Data handling
library(sf)
library(dplyr)
library(RCurl)
library(data.table)

# Paths gestion
library(here)

# Database connection
library(RPostgreSQL)
library(DBI)

# Graphes
library(ggplot2)

library(rmarkdown)

here::i_am("vignettes/Main_doc.Rmd")

```


# Introduction

The work of this internship consists of an analysis of the data of the NoiseCapture application. The data for this study are available through the use of SQL scripts preparing views on a backup of the global data of the application over the period from August 2017 to August 2020. The main theme of this study is the analysis of the distribution and use of tags, as well as the information that can be derived from this categorical information, on a temporal and spatial level.

The entirety of the SQL scripts, the RMarkdown documents and their resulting plots are freely viewable on my [GitHub](https://github.com/RustedPanda/lasso-data-analysis)


<!-- Knit child document for database analysis using SQL -->
```{r database-overview, child="[Analysis]Database_overview.Rmd", echo=FALSE, fig.align='center', out.width="80%", message = FALSE}
```

# Pre-processing of data

The data available is not usable as is. Many tracks have inconsistent or missing geographical data, while others have no tags. Some information necessary for our analyses is also missing, such as the country in which the track is located, its local time etc.

We have to clean unsuitable data and then calculate their other needed components.

We first want to conduct a temporal analysis, meaning that we won't apply any filter by surface of tracks for now.

From the *tracks_view* materialized view in our database, we are filtering tracks that are tagged "indoor" or "test", and with a duration under 5s. The only tracks considered are the tagged ones.

## Data cleaning

<!-- Knit child document for data cleaning -->
```{r data-cleaning, child="[Computing]Data_cleaning.Rmd", eval = !fast, echo=FALSE, fig.align='center', out.width="80%", message = FALSE, error = TRUE}
```

<!-- Write some things and show some results of the cleaning -->

## Adding useful components

<!-- Write some stuff to introduce -->

<!-- Knit child document to add components in our dataset -->
<!-- CHECK TIME AROUND SUNSET METHOD ~ WEIRD RESULTS -->
```{r calculate-comp, child=if (!fast) "[Computing]Calculate_components.Rmd", include = FALSE}
```

<!-- Knit child document to add weather in our dataset -->
```{r adding-weather, child=if (!fast) "[Computing]Adding_weather.Rmd", include = FALSE}
```

<!-- Knit child document to analyse weather -->
```{r weather-analysis, child= "[Analysis]Weather_analysis.Rmd", echo=FALSE, fig.align='center', out.width="80%", message=FALSE}
```

<!-- Knit child document to analyse hourly tag repartition -->
```{r hourly-tag-repartition, child= "[Analysis]Hourly_tag_repartition.Rmd", echo=FALSE, fig.align='center', out.width="80%",message=FALSE}
```

<!-- Knit child document to analyse repartition of tags around sunrise -->
```{r tags-sunrise, child= "[Analysis]Tags_around_sunrise.Rmd" , echo=FALSE, fig.align='center', out.width="80%",message=FALSE}
```


# Reproductibility

## Data sources

Most of the treatment has been made within the PostGIS database. The scripts folder contains several scripts to execute to prepare the dataset.

```{r spatial-data-info, echo = FALSE}


```

## Session informations

```{r session-info, echo=FALSE}
extract_loaded_package <- function(packages_info) {
  return(packages_info$Package) 
}

xfun::session_info(sapply(sessionInfo()$otherPkgs, extract_loaded_package), dependencies = FALSE)
```

## Database information

```{r pg-version}
# Check database connection and software versions
RPostgreSQL::dbGetQuery(con,statement = paste("SELECT version();")) # should return PostgreSQL 10.15 or higher
```

```{r postgis-version}
RPostgreSQL::dbGetQuery(con,statement = paste("SELECT postgis_full_version();")) # should return PostGIS 2.5 or higher
```

```{r close-connection, echo=FALSE}
RPostgreSQL::dbDisconnect(con)
```
