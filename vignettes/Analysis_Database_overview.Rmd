---
title: "[Analysis] Database overview"
subtitle: "Call to the database - SQL"
author: "Ludovic Moisan"
supervisors: "Pierre Aumond, Paul Chapron, Nicolas Roelandt"
date: "`r Sys.Date()`"
output: 
  html_document :
    theme: united
editor_options: 
  chunk_output_type: console
---

<!-- This file is for the plotting and analysis of our data distribution in our database -->

```{r library-database-overview, include=FALSE}

#Data handling
library(dplyr)

# Database connection
library(RPostgreSQL)
library(DBI)

# Graphes
library(ggplot2)

# handle figures and tables numbering
library(captioner)

```

```{r figures-caption-databases, include=FALSE}

# Declare variables for figures and figures captions

#Check first if figure numeration is already established by another child document to keep tracks of precedent numerations
if(!exists("fig_nums")){
fig_nums <- captioner::captioner(prefix = "Figure")
}

fig_nums("sql1", "Hourly spread of NoiseCapture usage", display = FALSE)
fig_nums("sql2", "Hourly spread of NoiseCapture tag function usage", display = FALSE)
fig_nums("sql3", "Hourly proportion of NoiseCapture tag function usage", display = FALSE)

```

```{r con-parameters, include = FALSE}
drv <- DBI::dbDriver("PostgreSQL")
con <- DBI::dbConnect(
drv,
dbname ="noisecapture",
host = "lassopg.ifsttar.fr", #server IP or hostname
port = 5432, #Port on which we ran the proxy
user="noisecapture",
password= Sys.getenv('noisecapture_password') # password stored in .Renviron. Use this to edit it : usethis::edit_r_environ()
)

```

## Database overview

Our database is derived from the recordings collected by the NoiseCapture application from 2017 to 2020, particularly those with a tag which has been manually entered by the user, determining the type of sound heard.
The first step in our analysis of this database was to check the repartition, accuracy and relevance of the said tags, whether it be for future work of data prediction or environmentâ€™s analysis.

The first study to be carried out within the framework of this analysis is the overview of the temporal distribution of the users' data. Only the UTC time is retrieved during the measurement by the application, so we need to convert this UTC time into local time based on the geographical area of the measurement in order to perform our analysis. Considering this, only the recordings having a geographical measurement could be taken into account in this study. A loss of about 36% of the data is thus to be taken into account (localization not activated by the user for the time of the recording for example). This subset of our data does include both tagged and untagged tracks.

The views are created on the POSTGRE database by the following SQL scripts : [link]

```{r analysis-sql1, echo = FALSE, fig.align='center', out.width="80%", message = FALSE, fig.cap= fig_nums("sql1")}

#Get all tracks and classify them by local_hour. Are considered only tracks with geographical data (around 64% of tracks only [100 - 94938 / 260422 * 100])

query <- "SELECT local_hour, COUNT(*) as Count_hour FROM tracks_countries GROUP BY local_hour ORDER BY local_hour"

count_tracks <- RPostgreSQL::dbGetQuery(con,statement = query)

ggplot(count_tracks) +
  aes(x = local_hour, y = count_hour) +
  geom_bar(stat = "identity") +
  geom_text(aes(label=count_hour), vjust=0-0.5, color = "blue3") +
  labs(
    x = "Local hour",
    y = "Count",
    title = "Repartition of tracks by local hour",
    subtitle = "Noisecapture's data,
      2017-2020"
  )
  #facet_wrap(vars(admin))

```

The graph above shows us the temporal distribution of the data on a 24h scale. It appears obvious that the use of the application follows the hours of human activity.

As mentioned earlier, the measurements can be divided into two categories, the tracks with tags indicating the nature of the noise recorded (animals, roads etc.) and those without tags. We want to know the distribution of use of this feature.

```{r analysis-sql2, echo=FALSE, fig.align='center', out.width="80%", message = FALSE,fig.cap= fig_nums("sql2")}

query <- "SELECT  local_hour, COUNT(*) as count_hour_tag FROM unique_tagged_tracks GROUP BY local_hour ORDER BY local_hour"

count_tags <- RPostgreSQL::dbGetQuery(con,statement = query)

ggplot(count_tags) +
  aes(x = local_hour, y = count_hour_tag) +
  geom_bar(stat = "identity") +
  geom_text(aes(label=count_hour_tag), vjust=-0.5, color = "blue3") +
  labs(
    x = "Local hour",
    y = "Count",
    title = "Repartition of tagged tracks by local hour",
    subtitle = "Noisecapture's data,
      2017-2020"
  ) 
  #facet_wrap(vars(admin))

```

The graph above shows us that the dynamics of tag usage is following the global usage of the application previously illustrated.

At first sight, it appeared that our analyses could be biased by the global hourly use when we calculate the percentage of the distribution of the different tags according to the hours of the day. Indeed, the hours of high use could comprise a greater percentage of tagged measurements, or conversely, during hours of low use.
In order to verify the limits of interpretation of our data, it is therefore important to verify this hypothesis by calculating the percentage of tagged tracks in relation to the total data over all our hours.

```{r visu-data, echo=FALSE, fig.align='center', out.width="80%", message = FALSE, fig.cap= fig_nums("sql3")}

df_test <- inner_join(count_tags, count_tracks)

df_test <- df_test %>% dplyr::mutate(percentage = count_hour_tag * 100 / count_hour)

ggplot(df_test) +
  aes(x = local_hour, y = percentage) +
  geom_bar(stat = "identity") +
  geom_text(aes(label= round(percentage,1)), vjust=-0.5, color = "blue3") +
  labs(
    x = "Local hour",
    y = "Percentage of tagged tracks",
    title = "Percentage of tagged tracks by local hour",
    subtitle = "Noisecapture's data,
      2017-2020"
  ) 

```

It appears on `r fig_nums("sql3", display="cite")` that the tagged tracks are rather evenly distributed over the hours of the day, with **a low standard deviation of `r round(sd(df_test$percentage),2)`%.**

`r fig_nums("sql3", display="cite")` tends to show that the use of tags does not depend much on local time, and can therefore be considered as independent of the global hourly use of the application for our future analyses. In concrete terms, it appears that we can divide the number of tags per hour either by the total number of traces or by the total number of tagged traces, without this making much difference in our visualizations and analyses.